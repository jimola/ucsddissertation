\section{Results for Input Poisoning Attacks}\label{chap4-sec:input-attacks}
So far we have only considered response poisoning attacks where the malicious users are free to report arbitrary responses to the data aggregator. However, to carry out such an attack in practice, a user would have to bypass the \ldp~data collection mechanism. Concretely, if a mobile application were used to collect a user's data, a malicious user would have to hack into the software and directly report their poisoned response. On the other hand, for input poisoning the malicious user needs to just lie about their input  to the application (for instance, by misreporting their list of friends) which is always possible. Hence clearly, input poisoning attacks are more easily realizable in practice. In fact, in certain cases the malicious users might be restricted to just input poisoning attacks due to the implementation of the \ldp~mechanism. For instance, mobile applications might have strict security features in place preventing unauthorized code tampering.  Another possibility is cryptographically verifying the randomizers~\cite{Kato21} to ensure that all the steps of the privacy protocol (such as, noise generation) is followed correctly.  Given its very realistic practical threat, in this section we study the impact of input poisoning attacks. 

Note that input poisoning attacks are strictly weaker than response poisoning attacks. This is because the poisoned input is randomized  to satisfy \ldp~in the former which introduces noise in the final output, thereby weakening the adversary's signal. Hence intuitively, we hope to obtain better robustness against input poisoning attacks.  In what follows, we formalize the above intuition for degree estimation. We first investigate the baseline protocols \RLap{} and \DegRRNaive{} and show that while input poisoning attacks are less damaging than response poisoning attacks, the protocols still suffer from poor robustness guarantees. Next, we show that our proposed protocols, \DegRRCheck{} and \DegHybrid{}, offer improved robustness against input poisoning attacks. These results demonstrate a separation between the efficacy of response and input poisoning attacks.

Recall in the Laplace mechanism, each user simply reports a private estimate of their degree. Under input poisoning attacks, Laplace noise is added to the poisoned input before it is reported to the data aggregator. Consequently, the response poisoning attack in which a malicious user could \textit{deterministically} report their degree as $n-1$ (Thm.~\ref{chap4-thm:response:laplace}) is no longer possible -- in order to manipulate their degree by $n-1$, the malicious user needs to get lucky with the sampled Laplace noise, resulting in the following theorem:

 \begin{thm} Let $\calM$ be a set of malicious users with $|\calM| = m$. The \RLap~protocol is $(\frac{1}{\epsilon}\ln\frac{n}{\delta},\delta)$ correct and $(n-1,\frac{1}{2})$-sound with respect to any input poisoning attack from $\calM$. \label{chap4-thm:input:laplace}\end{thm}
The proof of the above theorem is in App.~\ref{chap4-app:thm:input:laplace}.
Unsurprisingly, compared to Thm.~\ref{chap4-thm:response:laplace} for response poisoning attacks, the correctness guarantee is unchanged because no attack is possible for honest users. However, the soundness guarantee is different. Thm.~\ref{chap4-thm:response:laplace} delineates an $(n-1)$-tight soundness guarantee, demonstrating the feasibility of the worst-case attack in which a malicious user can \textit{always} manipulate their degree by $n-1$. In contrast, \RLap{} is $(n-1, \frac{1}{2})$-sound with respect to any input poisoning attack. This is because the sampled Laplace noise is negative with probability $\frac{1}{2}$.  Hence, even a worst-case malicious user who sends the maximum degree estimate of $n-1$ will only get assigned a final estimate this high if the sampled noise is non-negative. Thus, the noise from the Laplace mechanism prevents the adversary from carrying out the deterministic worst-case attack.

Next, we show the result for $\DegRRNaive{}$, our second baseline protocol. There is an improvement in both correctness and soundness, because the adversary's signals in the poisoned data (such as, a malicious user indicating they share an edge with every other user, or $m$ malicious users intentionally deleting their edges to an honest user), are noised via randomized response which weakens them.

\begin{thm}\label{chap4-thm:b2a2_easy}  Let $\calM$ be a set of malicious users with $|\calM| = m$. The \DegRRNaive{} protocol is $(m+\sqrt{n}\frac{\sqrt{2(e^\epsilon+1) \ln \frac{4n}{\delta}}}{e^\epsilon-1},\delta)$-correct and $(n-1,\frac{1}{2})$-sound with respect to any input poisoning attack from $\calM$.\label{chap4-thm:input:naive}
\end{thm}
The above theorem is proved in App.~\ref{chap4-app:thm:b2a2_easy}.
Written asymptotically, the correctness guarantee of Thm.~\ref{chap4-thm:input:naive} is $\tilde{O}(m + \frac{\sqrt{n}}{\epsilon})$, which improves the guarantee over response poisoning attacks (Thm.~\ref{chap4-thm:response:naive}) by a factor of $\frac{m}{\epsilon}$. This shows  a separation between input and response poisoning attacks.  A similar case holds for soundness -- while \DegRRNaive{} is $(n-1)$-tight sound under response poisoning attacks, for input poisoning attacks, it is $(n-1, \frac{1}{2})$-sound. The implications of this observation are similar to those of Thm.~\ref{chap4-thm:input:laplace} as discussed above.

%The soundness guarantee is  much better under an input poisoning attack, as a malicious user can only skew their estimate $\hd_i$ by $n-1$ with probability $1-\delta$ which is close to $\frac{1}{2}$ for small values of $\epsilon$. In other words, for input poisoning there is \textit{no} guarantee that a malicious user can \textit{always} get away with the worst-case skew of $n-1$.

%\par In conclusion, we observe that response poisoning attacks are more damaging than input poisoning attacks. 

Despite exhibiting improvement over response poisoning attacks, both naive protocols still fall short of providing acceptable soundness guarantees. Here, we analyze the robustness of our proposed protocols, \DegRRNaive{} and \DegHybrid{}, under input poisoning attacks. For both the mechanisms here, we are able to a set a smaller value for $\tau$, the threshold for checking the number of inconsistent edges. This is because the number of inconsistent edges is more concentrated around its means, and hence, a tighter confidence interval with a smaller $\tau$ suffices. Thus, both the correctness and soundness of the protocols are improved. Formally for \DegRRNaive{}, we have:

%\arc{We can catch here why - because otheruser gives me opposite data }
\begin{thm}\label{chap4-thm:input:check}
Let $\calM$ be a set of malicious users with $|\calM| = m$. 
The protocol $\DegRRCheck$ run with $\tau = m(1-2\rho) + \sqrt{8 \max\{\rho n, m\} \ln \frac{8n}{\delta}}$ is 
  $(2m+4\sqrt{\max\{n, m(e^\epsilon+1)\}}\frac{\sqrt{2(e^\epsilon+1) \ln \frac{8n}{\delta}}}{e^\epsilon-1}, \delta)$-correct and sound with respect to any input poisoning attack from $\calM$.
\end{thm}
The proof is in App.~\ref{chap4-app:b3a2}.
For typical values of $\epsilon$, the correctness and soundness guarantees can be written as $(\tilde{O}(m + \frac{\sqrt{n}}{\epsilon}), \delta)$ (because $\sqrt{m(e^\epsilon+1)} \leq \sqrt{n}$). Compared to Thm.~\ref{chap4-thm:response:check} for response poisoning attacks, there is an improvement of $\frac{m}{\epsilon}$ which is a direct consequence of a smaller $\tau$. 

For \DegHybrid{}, we have: 
\begin{thm}\label{chap4-thm:rrlapchecka2}
	Let $\calM$ be a set of malicious users with $|\calM| = m$. 
  For any $c \in (0,1)$, the \DegHybrid{} protocol run with threshold $\tau = m(1-2\rho) + \sqrt{8 \max\{\rho n, m\} \ln \frac{8n}{\delta}}$ is $(\frac{1}{(1-c)\epsilon}\ln \tfrac{4n}{\delta} ,\delta)$-correct and $(4m+8\sqrt{\max\{n, m(e^{c\epsilon}+1)\}}\frac{\sqrt{2(e^{c\epsilon}-1) \ln \frac{8n}{\delta}}}{e^{c\epsilon}+1}, \delta)$-sound with respect to any input poisoning attack from $\calM$. \label{chap4-thm:input:hybrid}
\end{thm}
This theorem is proved in App.~\ref{chap4-app:thm:rrlapchecka2}.
Written asymptotically, the correctness guarantee of \DegHybrid{} is $(\tilde{O}(\frac{1}{\epsilon}), \delta)$, and its soundness is $(\tilde{O}(m + \frac{\sqrt{n}}{\epsilon}), \delta)$.
Compared with Thm.~\ref{chap4-thm:response:hybrid} for response poisoning attacks, \DegHybrid{} offers similar correctness since the data aggregator uses the degree estimate collected via \RLap{} as its final estimate as before. However, the soundness guarantee  is improved by an additive factor of $O(\frac{m}{\epsilon})$, which comes from the smaller threshold $\tau$. %By collecting an additional degree estimate via the Laplace mechanism, \DegHybrid{} with the smaller choice of $\tau$ is able to provide better correctness than \DegRRCheck{} and improved soundness under input poisoning.

In conclusion, we observe that response poisoning attacks are more damaging than input poisoning attacks. In other words, our proposed degree estimation protocols offer better robustness against input poisoning attacks.


