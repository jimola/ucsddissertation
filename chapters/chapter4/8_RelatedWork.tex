  \vspace{-0.2cm}  \section{Related Work}\label{sec:relatedwork}
%In this section, we discuss the relevant prior work.\\\\
\noindent\textbf{Data Poisoning for LDP.}   A recent line of work~\cite{Cheu21,Cao21,Wu21,Li22} has explored the impact of poisoning attacks in the \ldp~setting. However, these works focused either on tabular data or key-value data. Additionally, prior work mostly focuses on the task of frequency estimation which is different from our problem of degree estimation. For the former, each user has some item from an input domain and the data aggregator wants to compute the histogram over all the users’ items. Whereas, we compute the degree vector $\langle \hat{d}_1, \ldots, \hat{d}_n\rangle$ -- each user directly reports their degree $d_i$ (a count or via an adjacency list). More specifically, Cao et al.~\cite{Cao21} proposed attacks where  an adversary could increase the estimated frequencies for adversary-chosen target items or promote them to be identified as heavy hitters. Wu et al.~\cite{Wu21} extended the attacks  for key-value data where an adversary
aims to simultaneously promote the estimated frequencies and mean values for some adversary-chosen target keys. Both the works also presented some ad-hoc defenses against the proposed attacks. However, their efficacy is heavily dependent on instantiation specific factors such as the data distribution or the percentage of fake users. Cheu et al.~\cite{Cheu21} formally analyzed the poisoning attacks on categorical data and showed that local algorithms are highly vulnerable to adversarial manipulation -- when the privacy level is high or the input domain is large, an adversary who controls a small fraction of the users in the protocol can completely obscure the distribution of the users’ inputs. This is essentially an impossibility result for robust estimation of categorical data via non-interactive LDP protocols. Additionally, they showed that poisoning the noisy messages can be far more damaging than poisoning the data itself. A recent work~\cite{Li22} studies the impact of data poisoning for mean and variance estimation for tabular data. In the shuffle \DP~model, Cheu et al.\cite{Cheu22} have studied the impact of poisoning on histogram estimation. %It is important to note that in addition to the type of data, the problem setting considered in prior work on LDP is also fundamentally different from ours.  
In terms of general purpose defenses, prior work has explored strategies strategy based on cryptographically  verifying implementations of \ldp~randomizers~\cite{Kato21,Ambainis03,Moran06} -- this would restrict the class of poisoning attacks to input poisoning only. \\A slew of poisoning attacks~\cite{biggio2021poisoning,mei2015teaching,Fang2020LocalMP,bhagoji2019, chen2017targeted,bagdasaryan2018backdoor,Xie2020DBADB} on machine learning (ML) models have been proposed in the federated learning setting~\cite{kairouz2019advances}. %The attacks could be either untargeted where the attack goal is to reduce the overall model accuracy~\cite{}, or targeted where backdoors are injected for certain target classes~\cite{}. 
A recent line of work \cite{roychowdhury2021eiffel, burkhalter2021rofl} explores defense strategies based on zero-knowledge proofs that aim to detect the poisoned inputs. Note that the aforementioned poisoning attacks on ML models are fundamentally different from the ones in our setting. For ML, the users send parameter gradient updates (multi-dimensional real-valued vector) and the attack objective is to misclassify data. On the other hand, our input here is a binary vector/single integral value with an underlying graphical structure and the attack objective is to perturb the degree estimates. Hence, none of the techniques from this literature are directly applicable to our setting.
  \vspace{-0.2cm}  \\\\
\noindent\textbf{LDP on Graphs.} DP on graphs has been widely studied, with most prior work being in the centralized model \cite{Graph1,Graph2,Graph3,Graph4,Graph5,Graph6,Graph7,Graph8,Graph9,Graph10}. 
In the local model, there exists work exploring the release of data about graphs in various settings ~\cite{LDPGraph1,LDPGraph2,LDPGraph3,LDPGraph4,LDPGraph5,LDPGraph6}. Recent work exploring private subgraph counting~\cite{imola2021locally,imola_communication-efficient_2022} is the most relevant to our setting, as the degree information is the count of an edge, the simplest subgraph. However, no previous work explores poisoning \ldp~protocols in the graph setting.
