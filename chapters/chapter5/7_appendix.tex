\section{Related Work}\label{app:related}

\paragraph{Differential Privacy} 
Differential privacy~\citep{dwork2006calibrating} has recently become the gold standard of privacy used by institutions such as the US census~\citep{dwork2019differential} and large tech companies~\citep{erlingsson2014rappor}. In a nutshell, DP algorithms provide plausible deniability for the input data of any user.   
There is a vast literature on DP algorithms for a disparate range of problems and many different models for differential privacy~\citep{dwork2006calibrating,mcsherry2007mechanism,chaudhuri2011differentially,roy2020crypte,machanavajjhala2017differential,dwork2019differential} (we refer to~\citet{dwork2014algorithmic} for a survey). 

Among this rapidly growing literature, our work builds on multiple work on differentially privacy, namely DP PCA algorithms~\citep{dwork2014analyze}, DP Johnson Lindenstrauss projections~ \citep{blocki2012johnson}, DP cut sparsification in graphs~\citep{eliavs2020differentially} as well as DP stochastic block model reconstruction (reviewed later). 

\paragraph{Private graph algorithms}
Especially relevant to this work is the area of differential privacy in graphs. DP has been  declined in graph problems both as  the edge-level~\citep{epasto2022differentially,eliavs2020differentially} and node-level  model~\citep{kasiviswanathan2013analyzing}. The most related work in this area is that on graph cut approximation~\citep{eliavs2020differentially,arora2019differentially}, as well as that of graph clustering with DP in correlation clustering model~\citep{bun2021differentially, cohen2022near}. 

\paragraph{Hierarchical Clustering}
As we discussed in the introduction, hierarchical clustering has been studied for decades in multiple fields. For this reason, a significant number of algorithms for hierarchical clustering have been introduced~\citep{murtagh2012algorithms}. Up until recently~\citep{dasgupta2016cost}, most work on hierarchical clustering has been heuristic in nature, defining algorithms based on procedures without specific theoretical guarantees in terms of approximation. Most well-known among such algorithms are the linkage-based ones~\citep{J10,bateni2017affinity}. \citet{dasgupta2016cost} introduced for the first time a combinatorial approximation objective for hierarchical clustering which is the one studied in this paper. Since this work, many authors have designed algorithms for variants of the problem~\citep{cohen2017hierarchical,cohen2019hierarchical, charikar2017approximate,moseley2017approximation, agarwal2022sublinear,chatziafratis2020bisect} exploring maximization/minimization versions of the problem on dissimilarity/similarity graphs.

Limited work has been devoted to DP hierarchical clustering algorithms. One paper~\citep{xiao2014differentially} initiates private clustering via MCMC methods, which are not guaranteed to be polynomial time. Follow-up work~\citep{kolluri2021private} shows that sampling from the Boltzmann distribution (essentially the exponential mechanism~\citep{mcsherry2007mechanism} in DP) produces an approximation to the maximization version of Dasgupta's function, which is a different problem formulation. Again, this algorithm is not provably polynomial time.

\paragraph{Private flat clustering}
Contrary to hierarchical clustering, the area of private {\it flat} clustering on metric spaces has received large attention. Most work in this area has focus on improving the privacy-approximation trade-off~\citep{ghazi2020differentially,balcan2017differentially} and on efficiency~\citep{hegde2021sok,cohennear,cohen2022scalable}.

\paragraph{Stochastic block models}

The Stochastic Block Model (SBM) is a classic model for random graphs with planted partitions which has received  significant attention in the literature. Most work in this area has focus on providing exact or approximate recovery of communities for increasingly more difficult regimes of the model~\citep{MR3520025-Guedon16,montanari2016semidefinite, moitra2016robust,MR4115142,ding2022robust,Liu-Moitra-minimax}. Specifically for our work, we focus on a variant of the model which has nested ground-truth communities arranged in a hierarchical fashion. This model has received attention for hierarchical clustering~\citep{cohen2017hierarchical}.   

The study of private algorithms for SBMs is instead very recent and no work has addressed private recovery for hierarchical SBMS. One of the only results known for private (non-hierarchical) SBMs is the work of~\citet{seif2022differentially} which provides a quasi-polynomial time algorithm for some regimes of the model. This paper require either non-poly time or $\epsilon \in \Omega(\log(|V|))$. 
Finally, very recently and currently to our work, the manuscript of~\citet{chen2023private} has been published. This work provides strong approximation guarantees using semi-definite programming for recovering SBM communities.    
None of these papers can be used directly to approximate hierarchical clustering on HSBMs. For this reason in Section~\ref{sec:algorithms-hsbm} we design a hierarchical clustering algorithm (Algorithm~\ref{alg:priv-hc-hsbm}) which uses as subroutine a DP SBM community detection algorithm. Moreover, we show a novel algorithm for SBMs (Algorithm~\ref{alg:priv-hsbm}) (independent to that of~\citet{chen2023private}) which is of practical interest as it does not require procedure with large polynomial dependency on the size of the input, such  solving a complex semi-definite program.

\section{Omitted proofs from Section~\ref{sec:lower-bounds}}\label{app:lower-bounds}
\subsection{Proof of Lemma~\ref{lem:packing-2}}

We start with the following lemma:

\begin{lem}\label{lem:two-packing}
    Let $G_1, G_2$ be two graphs drawn uniformly at random from $\calP(n,5)$. Let $\alpha = \frac{1}{100}$. The probability that there exists a balanced cut $(A,B)$ which misses at most $ \frac{\alpha}{5}n$ of the cycles for both $G_1, G_2$ is at most $2^{-0.4n}$.
\end{lem}
\begin{proof}
Let $(A,B)$ be any balanced cut with $|A| = \beta n$, for $\frac{1}{3} \leq \beta \leq \frac{2}{3}$. Let $\calE_1(A,B)$ be the event that $(A,B)$ misses at most $\frac{\alpha}{X}n$ cycles in $G_1$, and define $\calE_2(A,B)$ similarly for $G_2$. We observe the desired probability can be upper bounded by
\begin{equation}\label{eq:total-prob}
    \sum_{\substack{(A,B) \text{ a balanced cut}}} \Pr[\calE_1(A,B)] \Pr[\calE_2(A,B)].
\end{equation}

In the above sum, the balanced cuts $(A,B)$ are fixed, and the graphs $G_1, G_2$ are generated independently. We consider an equivalent random process, where $G_1 \in \calP(n,5)$ is fixed, and then $(A,B)$ is generated by picking a uniformly random string $S \in \{0,1\}^n$ with $\beta n$ $1$s. There are $\binom{n}{\beta n}$ possible strings. We will now upper bound the number of strings for which $\calE_1(A,B)$ holds. When $\calE_1(A,B)$ holds, we can choose $c$ cycles which are monochromatic $1$s, where $c$ is a non-negative integer such that $5c < n$, plus $\frac{\alpha n}{5}$ cycles which are not necessarily monochromatic. Within these $\frac{\alpha n}{5}$ cycles, there are $\alpha n$ vertices from which we can choose $d \leq \alpha n$ remaining $1$s. The total number of $1$s is $5c + d$, and thus $5c + d = \beta n$. Thus, the total number of admissible strings is at most 
\[
    \sum_{5c + d = \beta n, d \leq \alpha n}\binom{n/5}{c} \binom{n/5}{\alpha n / 5} \binom{\alpha n}{d}.
\]
We make the simple observation that $\binom{\alpha n}{d} \leq 2^{\alpha n}$. Furthermore, we observe that there are $\frac{\alpha n}{5}$ admissible choices of $c,d$. In the following, we use the fact that $2^{H_2(\beta) n - \ln n} \leq \binom{n}{\beta n} \leq 2^{H_2(\beta) n}$, where $H_2(p)$ is the binary entropy function. We upper bound the number of admissible strings with
\begin{align*}
    \frac{\alpha n}{5} \max_{(\beta-\alpha) n \leq 5c \leq \beta n} \binom{n/5}{c} \binom{n/5}{\alpha n / 5} 2^{\alpha n} &\leq 
    \frac{\alpha n}{5} \max_{(\beta - \alpha) n \leq 5c \leq \beta n} 2^{H_2(5c/n)n/5} 2^{H_2(\alpha)n/5}2^{\alpha n} \\ 
    &\leq n 2^{H_2(\beta)n/5} 2^{H_2(\alpha)n/5} 2^{\alpha n}.
\end{align*}
Dividing this number by $\binom{n}{\beta n}$, the total possible number of strings, we obtain
\begin{align*}
    \Pr[\calE_1(A,B)] &\leq \frac{n 2^{(H_2(\beta) + H_2(\alpha)) n / 5 + \alpha n}}{2^{H_2(\beta)n - \ln n}} \\
    &\leq 2^{\left(\frac{H_2(\beta) + H_2(\alpha)}{5} + \alpha - H_2(\beta) \right) n  + \ln n } \\
    &\leq 2^{-0.7 n},
\end{align*}
where the last line follows from the fact that $\frac{1}{3} \leq \beta \leq \frac{2}{3}$ and that $\alpha = \frac{1}{100}$ so that $H_2(\alpha) \leq 0.081$.
By a similar argument, we have $\Pr[A_2(B)] \leq 2^{-0.7 n}$.

Thus,~\eqref{eq:total-prob} can be upper bounded by
\[
    2^{n} \Pr[\calE_1(A,B)] \Pr[\calE_2(A,B)] \leq 2^{n} 2^{-2 \times 0.7 n} \leq 2^{-0.4n}.
\]

\end{proof}

Having shown the result for two random graphs, we apply the union bound to show that for exponentially many random graphs, it is unlikely that any tree can cluster more than one graph in the family well. We now prove Lemma~\ref{lem:packing-2}.

\begin{proof}
    Let $\calF$ consist of $2^{0.2n}$ graphs generated uniformly at random $\calP(n,5)$. For each pair of graphs $G_1, G_2$, we have by Lemma~\ref{lem:two-packing} every balanced cut will miss at least $\frac{\alpha}{5} n$ cycles in either $G_1$ or $G_2$ with probability $1-2^{-0.4n}$. By the union bound applied $\frac{1}{2}2^{0.4n}$ times for each pair of graphs, we have with probability $\frac{1}{2}$ that every balanced cut will miss at least $\frac{\alpha}{5} n$ cycles in all but at most one graph in $\calF$.
    
    Every tree can be mapped to a balanced cut, so by Lemma~\ref{lem:clique-miss-bad}, any tree will cost at least $\frac{4\alpha}{15}n^2 \geq \frac{n^2}{400}$ on all but at most one member of $\calF$. This allows us to conclude that the sets $\calB(G,r)$ are disjoint for all $G \in \calF$.
\end{proof}


\section{Omitted proofs from Section~\ref{sec:algorithms}}

\subsection{Proof of Theorem~\ref{thm:dp-spectral-sparse}}
First, we state a theorem about private graph sparsification.
\begin{thm}\label{thm:dp-spectral-sparse}
    There is a polynomial-time, $(\epsilon, \delta)$-edge differentially private algorithm which, on input graph $G = (V, E, w)$, outputs a graph $G'$ which with probability $0.9$ is a $(z, O(nz))$-approximation to cut queries in $G$, where $z = O(\frac{\log^2 \frac{1}{\delta}}{\epsilon}\frac{\log n}{\sqrt{n}})$.
\end{thm}
\begin{proof}
We apply an edge sparsification algorithm of~\citet{arora2019differentially}, which given a graph with Laplacian $L$, outputs a graph with Laplacian $L'$ with $O(\frac{n}{\gamma^2})$ edges such that
\[
    (1-\gamma) ((1-z)L + z L_n) \preceq L' \preceq (1+\gamma) ((1-z)L + z L_n),
\]
where $L_n$ is the Laplacian of an unweighted $K_n$. The value of the cut $w(S, \overline{S})$ is given by by $\textbf{1}_S^T L \textbf{1}_S$; therefore, we have
\begin{align*}
    (1-\gamma) ((1-z) w(S, \overline(S)) - z |S|(n-|S|)) &\leq w'(S, \overline{S}) \leq (1+\gamma) ((1-z) w(S, \overline{S}) + z |S|(n-|S|))
\end{align*}

Using the fact that $|S|(n-|S|) \leq n \min \{|S|, n-|S|\}$ and letting $\gamma \rightarrow 0$, we estabish that $G'$ is a $(z, n z)$ approximation to cut queries in $G$.
\end{proof}
Next, we reduce the cost to a sum of cuts. This idea appeared in~\citet{agarwal2022sublinear}. 
\begin{lem}\label{lem:hc-cuts}
    Suppose $G'$ is an $(\alpha_n, \beta_n)$-approximation to cut queries in $G$ for some $\alpha < 1$. Let $T'$ be any tree which satisfies $\cost_{G'}(T') \leq a_n \cost_{G'}^*$. Then,
    \[
        \cost_G(T') \leq (1+2\alpha_n) a_n \cost_G^* + (4a_n + 2) \beta_n n^2.
    \]
    For the revenue objective, let $T'$ be any tree which satisfies $\cost_{G'}^{\mw}(T') \geq a_n \cost_{G'}^{\mw*}$. Then,
    \[
        \cost_G^{\mw}(T') \geq (1-2\alpha_n) a_n \cost_{G}^{\mw*} - 2(a_n+1)\beta_n n^2 - 2(a_n+1)\alpha_n n^3.
    \]
\end{lem}
A proof of this lemma appears in the next section.

Finally, we are ready to prove the theorem.
\begin{proof} (Of Theorem~\ref{thm:dp-spectral-sparse}): 
    First, release a private graph $G'$ using Theorem~\ref{thm:dp-spectral-sparse}, which is a $(z, nz)$-cut approximation with probability at least $0.9$, where $z = O(\frac{\log^2 \frac{1}{\delta}}{\epsilon}\frac{\log n}{\sqrt{n}})$. We use the black box hierarchical clustering algorithm, which finds a tree such that $\E[\cost_G(T')] \leq a_n \cost_G^*$. Then, we apply Lemma~\ref{lem:hc-cuts}, obtaining
    \[
        \E[\cost_G(T')] \leq (1+2z) a_n \cost_G^* + (4a_n + 2) z n^3.
    \]
    For the revenue objective, our black box hierarchical clustering finds a tree $T'$ such that $\E[\cost_G^{\mw}(G')] \geq a_n \cost_G^{\mw*}$. We apply Lemma~\ref{lem:hc-cuts}, obtaining
    \[
        \cost_G^{\mw}(T') \geq (1-2z) a_n \cost_{G}^{\mw*} - 4(a_n+1)z n^3.
    \]
\end{proof}

\subsection{Proof of Lemma~\ref{lem:hc-cuts}}\label{app:hc-cuts}
    We start with the well-known representation of $\cost_G(T)$~\citep{dasgupta2016cost}:
    \[
        \cost_G(T) = \sum_{S \rightarrow (S_1, S_2) \text{ in $T$}} |S|w(S_1, S_2),
    \]
    where the sum is indexed by internal splits of $T$, which splits a set $S$ of leaves into two parts $S_1, S_2$. Using the identity $w(S_1, S_2) = \frac{1}{2}w(S_1, \overline{S_1}) + \frac{1}{2}w(S_2, \overline{S_2}) - \frac{1}{2}w(S, \overline{S})$, we substitute:
    \begin{align*}
        \cost_G(T) &= \frac{1}{2}\sum_{S \rightarrow (S_1, S_2) \text{ in $T$}} |S|w(S_1, \overline{S_1}) + |S|w(S_2, \overline{S_2}) - |S|w(S, \overline{S})
    \end{align*}
    In the above sum, if we assign cuts to their respective nodes, then we obtain the following: The root node is assigned $-|S|w(S, \overline{S}) = 0$. Each internal node $S_1$ which is not a leaf node or the root is assigned $|S|w(S_1, \overline{S_1}) - |S_1|w(S_1, \overline{S_1}) = |S_2| w(S_1, \overline{S_1})$, where $S \rightarrow (S_1, S_2)$ is the parent split of $S_1$. Finally, each leaf node $S_1$ is assigned $|S|w(S_1, \overline{S_1}) = |S_2|w(S_1, \overline{S_1}) + w(S_1, \overline{S_1})$, using the fact that $|S_1| = 1$. This brings us to the following decomposition~\citep{agarwal2022sublinear}:
    \[
        \cost_G(T) = \underbrace{\sum_{S \rightarrow (S_1, S_2) \text{ in $T$}}|S_2| w(S_1, \overline{S_1}) + |S_1| w(S_2, \overline{S_2})}_{\cost_G^1(T)} + \underbrace{\sum_{i = 1}^n w(v, \overline{v})}_{\cost_G^2}.
    \]
    We refer to the leftmost term of the above as $\cost_G^1(T)$, and the rightmost term as $\cost_G^2$. Observe the second quantity does not depend on $T$. Now, for any tree $T$, we have
    \begin{align*}
        \cost_{G'}^1(T) &\leq \sum_{S \rightarrow (S_1, S_2) \text{ in $T$}}\Big( |S_2| ((1+\alpha_n)w_{G}(S_1, \overline{S_1}) + \beta_n \min\{|S_1|, n - |S_1|\}) \\ &\qquad + |S_1| ((1+\alpha_n )w_{G}(S_2, \overline{S_2}) + \beta_n \min\{|S_2|, n - |S_2|\})\Big) \\
        &\leq (1+\alpha_n)\cost_{G}^1(T) + \beta_n \sum_{S \rightarrow (S_1, S_2) \text{ in $T$}} |S_2| \min\{|S_1|, n - |S_1|\} + |S_1| \min\{|S_2|, n - |S_2|\} \\
        &\leq (1+\alpha_n)\cost_{G}^1(T) + \beta_n \sum_{S \rightarrow (S_1, S_2) \text{ in $T$}} 2|S_1| |S_2| \\
        &\leq (1+\alpha_n)\cost_{G}^1(T) + \beta_n n^2,
    \end{align*}
    where the final line comes from an induction argument: if $f(n) \leq \max_{1 \leq i \leq n} f(i)f(n-i) + 2\beta i(n-i)$, then we can show via induction that $f(n) \leq \frac{n^2\beta}{2}$. By a similar process, we can show the following inequalities
    \begin{align}
    (1-\alpha_n) \cost_{G}^1(T) - \beta_n n^2 &\leq \cost_{G'}^1(T) \leq (1+\alpha_n) w_G^1(T) + \beta_n n^2 \label{eq:cost-decomp1}\\
    (1-\alpha_n) \cost_{G}^2 - \beta_n n &\leq \cost_{G'}^2 \leq (1+\alpha_n) \cost_{G}^2 + \beta_n n \label{eq:cost-decomp2}
    \end{align}
    This implies that
    \[
        (1-\alpha_n) \cost_{G}(T) - 2\beta_n n^2 \leq \cost_{G'}(T) \leq (1+\alpha_n) \cost_{G}(T) + 2\beta_n n^2.
    \]
    This allows us to derive that
    \begin{align*}
        \cost_G(T') &\leq (1+\alpha_n)\cost_{G'}(T') + 2\beta_n n^2 \\
        &\leq (1+\alpha_n) a_n \cost_{G'}(T^*) + 2\beta_n n^2 \\
        &\leq (1+\alpha_n) a_n ((1+\alpha_n) \cost_{G}^* + 2\beta_n n^2) + 2\beta_n n^2 \\
        &\leq (1+2\alpha_n) a_n \cost_{G}^* + (4a_n + 2) \beta_n n^2
    \end{align*}
    Plugging $T^*$, the optimal tree for $G$, into the above, we obtain that $\cost_{G'}^* \leq (1+\alpha_n)\cost_{G}^* + 2\beta_n n^2$, and therefore,
    \[
        \cost_{G'}(T') \leq a_n (1+\alpha_n) \cost_G^* + 2 a_n \beta_n n^2.
    \]
    We also have that $(1-\alpha_n) \cost_{G}(T') - 2\beta_n n^2 \leq \cost_{G'}(T')$, and we obtain our result by rearranging. 

\subsection{Proof of Lemma~\ref{lem:exp-util}}

Using a general lemma about the exponential mechanism~\citep{mcsherry2007mechanism}, we are able to prove a bound on the algorithm error.
\begin{lem}\label{lem:exp-util-appendix}
Let $f(X,Y)$ be a function with sensitivity $1$ in $X$.
Suppose we run the exponential mechanism $M : \calX \rightarrow \calY$ with finite range $\calY$ using utility function $u_X(Y) = f(X,Y)$. Let $OPT(X) = \min_{Y \in \calY} u_X(Y)$. If our privacy budget is $\epsilon$, then for each $X \in \calX$, we have
\[
    \Pr[u_X(M(X)) \leq OPT(X) + 2\frac{\log(|\calY|)}{\epsilon}] \geq 1-\frac{1}{|\calY|}.
\]
\end{lem}
\begin{proof}
    Let $\calZ = \{Y \in \calY : u_X(Y) \leq OPT(X) + 2\frac{\log(|\calY|)}{\epsilon}\}$. We are guaranteed that the optimal element, $Z^*$, with $u_X(Z^*) = OPT(X)$, is in $\calZ$. We want to lower bound the quantity $\Pr[M(X) \in \calZ]$. Observe that
    \begin{align*}
        \Pr[M(X) \in \calZ] &= \frac{\sum_{Z \in \calZ} e^{-\epsilon u_X(Z)/2}}{\sum_{Z \in \calZ} e^{-\epsilon u_X(Z)/2} + \sum_{Y \in \calY, Y \notin \calZ} e^{-\epsilon u_X(Y)/2}} \\
        &\geq \frac{e^{-\epsilon u_X(Z^*)/2}}{e^{-\epsilon u_X(Z^*)/2} + \sum_{Y \in \calY, Y \notin \calZ} e^{-\epsilon u_X(Y)/2}} \\
        &= \frac{e^{-\epsilon OPT(X)/2}}{e^{-\epsilon OPT(X)/2} + \sum_{Y \in \calY, Y \notin \calZ} e^{-\epsilon u_X(Y)/2}}.
    \end{align*}
    The second line holds because the function $g(z) = \frac{z}{z+K}$ for $K > 0$ is decreasing as $z \rightarrow 0$. The bottom sum can be upper bounded with $|\calY| e^{-\epsilon (OPT(X) + 2\log(|\calY|) / \epsilon)/2} \leq \frac{1}{|\calY|} e^{-\epsilon OPT(X)/2}$. Thus, we are left with
    \[
        \Pr[M(X) \in \calZ] \geq \frac{1}{1 + 1 / |\calY|} \geq 1-\frac{1}{|\calY|}.
    \]
\end{proof}
For hierarchical clustering, our algorithm is a corollary of the previous result:

\begin{proof}
We apply the exponential mechanism with utility function $u_G(T) = -\frac{1}{n} \cost_G(T)$, which has sensitivity $1$. The range of the algorithm is the space of trees with $n$ nodes; there are at most $n^n$ trees of this size. By Lemma~\ref{lem:exp-util-appendix}, the utility satisfies $\Pr[ \frac{\cost_G^*}{n} \leq \frac{\cost_G(M(G))}{n} + 2 \frac{n \log n}{\epsilon}] \geq 1-o(1)$, and hence the algorithm is a $(1, O(\frac{n^2 \log n}{\epsilon}))$-approximation. 

For the revenue objective, we apply the exponential mechanism with utility function $u_G(T) = \frac{1}{2n} \cost_G^{\mw}(T)$, which has sensitivity $1$. By Lemma~\ref{lem:exp-util}, the utility satisfies $\Pr[\frac{\cost_G^{\mw}(M(G))}{2n} \leq \frac{\cost_G^{\mw*}}{2n} + 2 \frac{n \log n}{\epsilon}] \geq 1-o(1)$. This establishes $(1, O(\frac{n^2\log n}{\epsilon}))$-approximation.
\end{proof}


\section{Omitted proofs from Section~\ref{sec:algorithms-hsbm}}

\subsection{Proof of Theorem~\ref{thm:hc-hsbm-util}}\label{sec:hc-hsbm-util}

In order to prove this theorem, we will show that \dphcblocks{} finds a $(1+o(1))$-approximate ground-truth tree, and then appeal to a result showing the such trees are approximately optimal with high probability~\citep{cohen2019hierarchical}:

\begin{lem}\label{lem:approx-hsbm-tree-opt} (Lemma 5.10 from~\citet{cohen2019hierarchical})
Let $G$ be a graph drawn from $\hsbm(B,P,f)$,  where $p_{min} = \min_{i \in B \cup N} f(i) \geq \omega(\sqrt{\frac{\log n}{n}})$. Let $(B,P',f')$ be a $\gamma$-approximate ground-truth tree. Then, with probability $1-2^{-n}$, we have
\[
    \dcost_G(P') \leq \gamma (1 + o(1)) \dcost_G^*,
\]
\end{lem}

We now show that \dphcblocks{} outputs an approximate ground-truth tree.
We introduce a high-probability event and prove that if it happens, then the output is an approximate ground-truth tree.

Our event $\calE$ states that $sim(B_i, B_j)$ as used in \dphcblocks{} is a good estimate for $f(LCA_P(B_i, B_j))$. Intuitively, this makes sense, as if one had access to $f(LCA_P(B_i, B_j))$, then it would be easy to construct $P$ (or an equivalent tree) using single linkage. Formally, we let $\calE$ denote the event that there exists $\alpha$ such that for all $B_i, B_j$,
\begin{equation}\label{eq:good-event}
    \big|sim(B_i, B_j) - f(LCA_P(B_i, B_j))\big| \leq \alpha f(LCA_P(B_i, B_j)).
\end{equation}

The following lemma shows that $\calE$ occurs with high probability.
\begin{lem}\label{lem:good-event}
If $|B_i| \geq n^{2/3}$ for all $i, j$, $\epsilon \geq \frac{1}{n^{1/2}}$, and $f(x) \geq \frac{\log n}{n^{1/2}}$, then the event $\calE$ occurs with $\alpha = \frac{8}{n^{1/6}}$ with probability at least $1-\frac{2}{n}$.
\end{lem}

\begin{proof}
The values $w_G(B_i, B_j)$ are distributed according to $\text{Binomial}(N_{ij}, p_{ij})$, where $N_{ij} = |B_i||B_j|$ and $p_{ij} = f(LCA_P(B_i, B_j))$. By Hoeffding's bound, we have that
\[
    \Pr[|w_G(B_i, B_j) - p_{ij}N_{ij}| \geq 2 \log n \sqrt{N_{ij}}] \leq \frac{1}{n^3}.
\]
Furthermore, we have that $\Pr[|\calL_{ij}| \geq \frac{6 \log n}{\epsilon}] \leq \frac{1}{n^3}$. Plugging in $sim(B_i, B_j) = \frac{w_G(B_i, B_j) + \calL_{ij}}{N_{ij}}$, we obtain
\[
    \Pr\left[|sim(B_i, B_j) - p_{ij}| \geq \frac{2 \log n}{\sqrt{N_{ij}}} + \frac{6 \log n}{\epsilon N_{ij}} \right] \leq \frac{2}{n^3}.
\]
Because $N_{ij} \geq n^{4/3}$ and $\epsilon \geq \frac{1}{n^{1/2}}$, we have $\frac{2 \log n}{\sqrt{N_{ij}}} + \frac{6 \log n}{\epsilon N_{ij}} \leq \frac{8 \log n}{n^{2/3}} \leq \frac{8}{n^{1/6}} p_{ij}$.
Thus, we obtain $\Pr[|sim(B_i, B_j) - p_{ij}| \geq \alpha p_{ij}] \leq \frac{2}{n^3}$, with $\alpha = \frac{8}{n^{1/6}}$. Taking a union bound over all $\binom{k}{2} \leq n^2$ choices of $i,j$, we obtain our result.
\end{proof}

Finally, we show that when $\calE$ occurs, then \dphcblocks{} finds an approximate ground-truth tree. A similar result was proved in~\citet{cohen2019hierarchical}, though our lemma statement is sufficiently different that we include a proof here.
\begin{lem}\label{lem:approx-ground-truth}
Assume that event $\calE$ occurs. Then, the tuple $(B, T, f')$ returned by Algorithm~\ref{alg:priv-hc-hsbm} is a $(1 + \alpha)$-approximate ground-truth tree for $(B, P, f)$.
\end{lem}
\begin{proof}
We want to show that for all $B_i, B_j \in V$, we have
\[
    (1-\alpha) f(LCA_P(B_i, B_j)) \leq f'(LCA_{P'}(B_i,B_j)) \leq (1+\alpha) f(LCA_P(B_i, B_j)).
\]
Let $I = LCA_{T}(B_i, B_j)$ be the internal node in which $B_i, B_j$ are merged, and let $C_i, C_j$ be the children of $I$ such that $B_i \subseteq C_i$ and $B_j \subseteq C_j$.
We have that
\[
    f'(LCA_{P'}(B_i, B_j)) = sim(C_i, C_j) = \max_{B \in C_i, B' \in C_j} sim (B, B').
\]
Thus, it holds that $sim(B_i, B_j) \leq f'(LCA_{P'}(B_i, B_j))$. As event $\calE$ holds, we have that $sim(B_i, B_j) \geq (1-\alpha)f(LCA_P(B_i, B_j))$.

To finish, we show that $sim(C_i, C_j) \leq (1+\alpha) f(LCA_P(B_i, B_j)$.
Let $J = LCA_P(B_i, B_j)$ be the internal node in which $B_i, B_j$ are merged in $P$, and let $D_i, D_j$ be the children of $J$ such that $B_i \subseteq D_i$ and $B_j \subseteq D_j$. We consider the following two cases.
\paragraph{Case 1:} $C_i \subseteq D_i$ and $C_j \subseteq D_j$.
Then, we have 
\[
    sim(C_i, C_j) \leq \max_{B \in D_i, B' \in D_j} sim(B, B') \leq (1+\alpha) \max_{B \in D_i, B' \in D_j} f(LCA_P(B, B')).
\]
As $D_i, D_j$ are nodes of the ground-truth tree, it holds that $f(LCA_P(B, B'))$ is the same for any choice of $B \in D_i, B' \in D_j$. In particular, this is true for $f(LCA_P(B_i, B_j))$.

\paragraph{Case 2:} There exists $B_\ell$ such that $B_\ell \subseteq C_i$ and $B_\ell \nsubseteq D_i$ (or the same holds for $C_i, D_i$ replaced by $C_j, D_j$).
WLOG, suppose the former case holds. Then, there exists a child $N$ of $I$ whose children are $N_L, N_R$, such that $N_L \subseteq D_i$ and $N_R \cap D_i = \emptyset$. It then follows that 
\[
    sim(N_L, N_R) \leq (1+\alpha) \max_{B \in N_L, B' \in N_R} f(LCA_P(B, B')) \leq (1+\alpha) f(LCA_P(B_i, B_j)),
\]
where the second inequality holds because $f$ is decreasing as we ascend $P$.
However, we also have that $sim(N_L, N_R) \geq sim(C_i, C_j)$, as $sim$ also obeys this property (if the last inequality did not hold, then $N_L, N_R$ would not have been merged). This finishes the last case.
\end{proof}

The proof follows by applying Lemma~\ref{lem:good-event} and then Lemma~\ref{lem:approx-ground-truth}.

\subsection{Proof of Theorem~\ref{thm:com-hsbm-priv}}\label{sec:com-hsbm-priv}
\subsubsection{Overview}
When running \dpcom{}, fix $Y,Z_1,Z_2$, and let $(\hat{A}_1, \hat{A}_2)$ and $(\hat{A}_1', \hat{A}_2')$ be the splits of $\hat{A}$ and an adjacent database $\hat{A}'$.
We will view the matrix $F = P(\Pi_{\hat{A}_1}^{(k)}(\hat{A}_2))$ as a vector, and then show that releasing $F$ plus appropriate Gaussian noise satisfies privacy via the Gaussian mechanism. Our proof will bound the $L_2$ sensitivity of $F$, given by
\[
    \Delta_2(F) = \|P(\Pi_{\hat{A}_1}^{(k)}(\hat{A}_2)) - P(\Pi_{\hat{A}_1'}^{(k)}(\hat{A}_2'))\|_F,
\]
in terms of the quantity $\Gamma = \frac{\sigma_1(\hat{A}_2)}{\sigma_k(\hat{A}_1) - \sigma_k(\hat{A}_2)}$. Recall that $P$ is a random $m \times \frac{n}{2}$ projection matrix. To control this sensitivity, we will need the fact that $P$ preserves the distances in $A$ via the Johnson-Lindenstrauss projection theorem:
\begin{thm}\label{thm:jl}
(Johnson-Lindenstrauss projection theorem~\citep{johnson1984extensions}): Let $0 \leq \alpha < \frac{1}{2}$ and $0 \leq \beta \leq 1$, and $m = 8\frac{ \ln \frac{2}{\beta}}{\alpha^2}$. If $x \in \R^n$ is a vector and $P \sim \calN(0, \frac{1}{\sqrt{m}})^{m \times n}$ is a random matrix then with probability $1-\beta$, we have
\[
(1-\alpha) \|x\|_2 \leq \|Px\|_2 \leq (1+\alpha) \|x\|_2
\]
\end{thm}
We use the above theorem to show that the matrix $P$ does not increase the sensitivity $\Delta(F)$ with high probability.
\begin{lem}\label{lem:proj-sens}
    Let $0 \leq \delta < 1$ and $m = 64 \ln \frac{2n}{\delta}$. Then, if $P \sim \calN(0, \frac{1}{\sqrt{m}})^{m \times n/2}$ the following holds with probability at least $1-\frac \delta 4$:
    \[
        \Delta(F) \leq \frac 3 2 \|\Pi_{\hat{A}_1}^{(k)}(\hat{A}_2) - \Pi_{\hat{A}_1'}^{(k)}(\hat{A}_2')\|_F.
    \]
\end{lem}
\begin{proof}
Let the columns of $\Pi_{\hat{A}_1}^{(k)}(\hat{A}_2)$ be $\{a_1, \ldots, a_{n/4}\}$ and the columns of $\Pi_{\hat{A}_1'}^{(k)}(\hat{A}_2')$ be $\{a_1', \ldots, a_{n'/4}\}$. By the union bound, Theorem~\ref{thm:jl} with $\alpha=\frac{1}{2}$ and $\beta = \frac{\delta}{n}$ applies to all vectors $a_i - a_i'$ with probability at least $1-\frac{\delta}{4}$. Thus, we have
\[
\Delta_2(F)^2 = \sum_{i=1}^{n/4} \|P(a_i) - P(a_i')\|_2^2 \leq (1+\alpha)^2 \sum_{i=1}^{n/4} \|a_i - a_i'\|_2^2 = (1+\alpha)^2  \|\Pi_{\hat{A}_1}^{(k)}(\hat{A}_2) - \Pi_{\hat{A}_1'}^{(k)}(\hat{A}_2')\|_F^2.
\]
The result follows.
\end{proof}
Finally, we need a bound on the stability of the projection $\Pi_{\hat{A}_1}^{(k)}$ when $\hat{A}_1$ is perturbed. This is the result of the Davis-Kahan Theorem~\citep{bhatia1997}.
\begin{thm}\label{thm:davis-kahan}
Let $\hat{A}_1, \hat{A}_1'$ be matrices where $d_k= \sigma_k(\hat{A}_1) - \sigma_{k+1}(\hat{A}_1) > 0$. Then, 
\[
    \|\Pi_{\hat{A}_1}^{(k)} - \Pi_{\hat{A}_1'}^{(k)}\|_F \leq \frac{\|\hat{A}_1 - \hat{A}_1'\|_F}{d_k}.
\]
Furthermore, the above holds replacing $\|\cdot\|_F$ with $\|\cdot \|_2$.
\end{thm}

Having bounded the $L_2$-sensitivity, we finally use the well-known Gaussian mechanism~\citep{dwork2014algorithmic}
\begin{thm}\label{thm:gauss-mech}
    If $x \in \R^m$ has $L_2$ sensitivity at most $S$, then releasing $x + N$, where $N \sim \frac{S}{\epsilon} \sqrt{2 \ln \frac{1.25}{\delta}}\calN(0,1)^{m}$ satisfies $(\epsilon, \delta)$-DP.
\end{thm}

\subsubsection{Proof}
Let $\hat{A}$ and $\hat{A}'$ be two adjacent inputs, and consider two runs of \dpcom{} with fixed $Y,Z_1,Z_2$, and $P$; we will show that the outputs satisfy $(\epsilon, \delta)$-DP. Let $\hat{A}_1'$ and $\hat{A}_2'$ be the values of $\hat{A}_1$ and $\hat{A}_2$ when $\hat{A}'$ is used instead of $\hat{A}$. \dpcom{} can be viewed as a post-processing of the private release of values $d_k = \sigma_k(\hat{A}_1) - \sigma_{k+1}(\hat{A}_1)$, $\sigma_1(\hat{A}_2)$, and $F$; thus, we will show that releasing each of these values satisfies privacy.

Using Lindskii's inequality~\citep{bhatia1997}, each rank $i$ singular value of $\hat{A}_1, \hat{A}_2$ can only change by $1$ when $\hat{A}$ is changed to $\hat{A}'$. Thus, the sensitivity of $d_k$ is $2$, of $\sigma_1$ is $1$, and thus the release of $\tilde{d}_k = d_k + \frac{8}{\epsilon} \ln \frac{4}{\delta} + Lap(\frac{8}{\epsilon})$ and $\tilde{\sigma}_1 = \sigma_1 + \frac{4}{\epsilon} \ln \frac{4}{\delta} + Lap(\frac{4}{\epsilon})$ both satisfy $(\frac{\epsilon}{4}, 0)$-DP. Thus, we will show that releasing $\tilde{F}$ satisfies $(\frac \epsilon 2, \delta)$-DP, and privacy will follow by composition.

By Lemma~\ref{lem:proj-sens} with probability at least $1-\frac{\delta}{4}$, we have
\[
    \Delta_2(F) \leq \tfrac 3 2 \|\Pi_{\hat{A}_1}^{(k)}(\hat{A}_2) - \Pi_{\hat{A}_1'}^{(k)}(\hat{A}_2')\|_F
\]
We have either $\hat{A}_1 = \hat{A}_1'$ or $\hat{A}_2 = \hat{A}_2'$. We analyze the cases separately.

\paragraph{Case $\hat{A}_1 = \hat{A}_1'$:} Then, $\hat{A}_2$ and $\hat{A}_2'$ differ in one bit, so $\hat{A}_2 = \hat{A}_2' + E$, where $E$ is a matrix that is $\pm 1$ in one entry and $0$ everywhere else.
Then,
\[
\tfrac 3 2\|\Pi_{\hat{A}_1}^{(k)}(\hat{A}_2) - \Pi_{\hat{A}_1}^{(k)}(\hat{A}_2')\|_F = \tfrac 3 2\|\Pi_{\hat{A}_1}^{(k)}(E)\|_F \leq \tfrac 3 2\|E\|_F \leq \tfrac 3 2,
\]
where the inequality holds because projecting vectors onto a subspace cannot increase their magnitude.

\paragraph{Case $\hat{A}_2 = \hat{A}_2'$:} Then, $\hat{A}_1$ and $\hat{A}_1'$ differ in one bit, so $\|\hat{A}_1 - \hat{A}_1'\|_F \leq 1$. We have
\[
\|\Pi_{\hat{A}_1}^{(k)}(\hat{A}_2) - \Pi_{\hat{A}_1'}^{(k)}(\hat{A}_2')\|_F \leq 2k \|(\Pi_{\hat{A}_1}^{(k)} - \Pi_{\hat{A}_1'}^{(k)})(\hat{A}_2)\|_2 \leq 2k\|\Pi_{\hat{A}_1}^{(k)} - \Pi_{\hat{A}_1'}^{(k)}\|_2\|\hat{A}_2\|_2,
\]
where the first inequality holds because each term has rank at most $k$, so the entire quantity has rank at most $2k$, and the  second holds by sub-multiplicativity of $\|\cdot\|_2$.
By Theorem~\ref{thm:davis-kahan}, we have $\|\Pi_{\hat{A}_1}^{(k)} - \Pi_{\hat{A}_1'}^{(k)}\|_2 \leq \frac{1}{d_k}$.
Thus, we have
\[
\frac 3 2 \|\Pi_{\hat{A}_1}^{(k)}(\hat{A}_2) - \Pi_{\hat{A}_1'}^{(k)}(\hat{A}_2')\|_F \leq \frac{3k\|\hat{A}_2\|_2}{d_k} = 3k\Gamma.
\]
By concentration of Laplace variables, we have $\tilde{d}_k \leq d_k$ and $\tilde{\sigma}_1 \geq \sigma_1$, so $\Gamma \leq \frac{\tilde{\sigma}_1}{\tilde{d}_k} = \tilde{\Gamma}$ with probability at least $1-\frac{\delta}{2}$. Thus, the sensitivity $\Delta(F)$ is at most $3k\tilde{\Gamma}$, and $(\tfrac \epsilon 2, \frac{\delta}{4})$-DP follows via Theorem~\ref{thm:gauss-mech}. Factoring in the aformentioned failure probabilities, the entire release of $\tilde{F}$ satisfies $(\tfrac \epsilon 2, \delta)$-DP.


\subsection{Proof of Corollary~\ref{thm:com-hsbm-util-inf}}\label{sec:com-hsbm-util}
\subsubsection{Overview}

Recall that \dpcom{} sees a matrix $\hat{A}$ drawn from $\hsbm(B,P,f)$, with expectation matrix $A$. We define $\tau^2 = \max f(x)$, $s = \min_{i=1}^k |B_i|$, and $\Delta=\min_{u \in B_i, v \in B_j, i \neq j} \|A_u - A_v\|_2$. 
We will show that \dpcom{} approximates $\Pi_{\hat{A}_1}^{(k)}(\hat{A}_2)$, which is guaranteed to cluster the original communities via the following result~\citep{vu2014simple}. We let the columns of $\Pi_{\hat{A}_1}^{(k)}(\hat{A}_2)$, which is indexed by the set $Z_2$, be $\{b_i : i \in Z_2\}$.

\begin{thm}\label{thm:svd-recovery} (\citet{vu2014simple}): There exists a universal constant $C$ such that if $\tau^2 \geq C \frac{\log n}{n}$, $s \geq C \log n$, and $k < n^{1/4}$.  $\Delta > C (\tau \sqrt{\frac{n}{s}} + \tau \sqrt{k \log n} + \frac{\tau\sqrt{nk}}{\sigma_k(A)})$, with probability at least $1 - n^{-1}$, then the columns $\{b_i : i \in Z_2\}$ in $\Pi_{\hat{A}_1}^{(k)}(\hat{A}_2)$ satisfy:
\begin{align*}
    \|b_{i} - b_{j}\|_2 &\leq \frac{\Delta}{4}  \ \ \ \text{if $\exists u.~i \in B_u, j \in B_u$ (i.e. $i,j$ are in the same community)} \\ 
    \|b_{i} - b_{j}\|_2 &\geq \Delta  \ \ \ \text{otherwise.}
\end{align*}
\end{thm}

Thus, the clusters in $\Pi_{\hat{A}_1}^{(k)}(\hat{A}_2)$ cluster the original communities assuming $\Delta$ is large enough. We will show that $\tilde{F}$ clusters the original communities assuming some condition on $\Delta$. Since
\dpcom{} returns $\tilde{F} = P(\Pi_{\hat{A}_1}^{(k)}(\hat{A}_2)) + N$, where $N$ is Gaussian noise, our proof involves showing that the distances in $\tilde{F}$ approximate those in $\Pi_{\hat{A}_1}^{(k)}$ using the Johnson-Lindenstrauss lemma and concentration of the Gaussian noise.

We formally restate Theorem~\ref{thm:com-hsbm-util-inf}:

\begin{thm}\label{thm:com-hsbm-util}
Let $\hat{A}$ be drawn from $\hsbm(B,P,f)$. There is a universal constant $C > 2000$ such that if $\tau^2 \geq C \frac{\log n}{n}$, $s \geq C \sqrt{n \log n}$, $k < n^{1/4}$, $\delta < \frac{1}{n}$, $\sigma_k(A) \geq C \max\{ \tau \sqrt{n}, \frac{1}{\epsilon} \ln \frac{4}{\delta}\}$, and 
\[\Delta > C\max\left\{\tfrac{ k (\ln \frac{1}{\delta})^{3/2}}\epsilon \tfrac{\sigma_1(A)}{\sigma_{k}(A)},  \tau \sqrt{\tfrac{n}{s}} + \tau \sqrt{k \log n} + \tfrac{\tau\sqrt{nk}}{\sigma_k}\right\},\]
then with probability at least $1 - 3n^{-1}$, \dpcom{} returns a set of points $\tilde{F} = \{f_i : i \in Z_2\}$ such that
\begin{align*}
    \|f_{i} - f_{j}\|_2 &\leq \frac{2\Delta}{5}  \ \ \ \text{if $\exists u.~i,j \in B_u$} \\
    \|f_{i} - f_{j}\|_2 &\geq \frac{4\Delta}{5}  \ \ \ \text{otherwise},
\end{align*}
and thus the clusters in $\tilde{F}$ indicate the communities.
\end{thm}

\subsubsection{Proof of Theorem~\ref{thm:com-hsbm-util}}
Let the columns of $\tilde{F}$ be $\{f_i : i \in Z_2 \}$.
We have $f_i = P(b_i) + n_i$, where $n_i \sim \frac{3k \tilde{\Gamma}}{\epsilon} \sqrt{2\ln \frac{5}{\delta}}N(0,1)^{m}$. By concentration bounds, we have with probability $1-\frac{1}{n}$ that each $n_i$ satisfies $\|n_i\|_2 \leq \frac{3k\tilde{\Gamma}}{\epsilon}\sqrt{2\ln \frac{5}{\delta}} \sqrt{2m \ln n} \triangleq K$. Next, applying Theorem~\ref{thm:jl} on the vectors $b_i - b_j$ for $i,j \in Z_2$, we have $0.9 \|b_i - b_j\|_2 \leq \|P(b_i) - P(b_j)\|_2 \leq 1.1 \|b_i - b_j\|_2$ with probability $1-\delta > 1-\frac{1}{n}$. Thus, if $\exists u.~ i \in B_u, j \in B_u$, then
\begin{align*}
    \|f_i - f_j\|_2 &\leq \|P(b_i) - P(b_j)\|_2 + \|n_i\|_2 + \|n_j\|_2 \\
    &\leq 1.1 \|b_i - b_j\|_2 + 2K \\
    &\leq 0.275 \Delta + 2K,
\end{align*}
Otherwise, we have
\begin{align*}
    \|f_i - f_j\|_2 &\geq \|P(b_i) - P(b_j)\|_2 - \|n_i\|_2 - \|n_j\|_2 \\
    &\geq 0.9 \|b_i - b_j\|_2 - 2K \\
    &\geq0.9\Delta - 2K.
\end{align*}


Finally, we show that $K$ can be upper bounded by the singular values of the expectation matrix $A$. This can be done with the following two lemmas which are proven implicitly in~\citet{vu2014simple}.
\begin{lem}\label{lem:rand-mat-sv}
Let $A$ be an $m \times n$ (with $m \geq n$) matrix of expectations in $[0,1]$, and let $\hat{A}$ be a randomized rounding of $A$ to $\{0,1\}$. Then, with probability at least $1 - \frac{1}{n}$, we have for all $1 \leq i \leq m$, $|\sigma_i(A) - \sigma_i(\hat{A})| \leq 4\tau \sqrt{n} + 4 \log n$, where $\tau^2$ is the maximum probability in $A$.
\end{lem}

\begin{proof}
Each $\sigma_{i+1}(A)$ is equal to $\max_{\text{rank}(A_{i}) = i} \|A - A_i\|_2$.
Let $A_i^*, \hat{A}_i^*$ be rank $i$ matrices such that $\sigma_{i+1}(A) = \|A - A_i^*\|_2$ and $\sigma_{i+1}(\hat{A}) = \|\hat{A} - \hat{A}_i^*\|_2$. We have that $\sigma_{i+1}(A) \leq \|A - \hat{A}_i^*\|_2 \leq \|\hat{A} - \hat{A}_i^*\|_2 + \|A - \hat{A}\|_2$. 

Thus, it remains to bound $\|A - \hat{A}\|_2$. Let the columns in $A - \hat{A}$ be $a_1, \ldots, a_n$. Using Lemma 7 from~\citet{vu2005spectral}, we have that with probability at least $1-\frac{1}{n^3}$, the length of the projection of $a_i$ onto a basis vector $e_i$ is at most $4(\tau + \frac{\log n}{\sqrt{n}})$. Thus, the total length $\|Ae_i\|_2$ is at most $4(\tau + \frac{\log n}{\sqrt{n}}$, and thus $\|A\|_2 \leq \sqrt{n}4(\tau + \frac{\log n}{\sqrt{n}})$ establishing that $\sigma_{i+1}(A) \leq \sigma_{i+1}(\hat{A}) + 4\sqrt{n}\tau + 4 \log n$. Likewise, we can show that $\sigma_{i+1}(A) \geq \sigma_{i+1}(\hat{A}) - 4\sqrt{n}\tau - 4 \log n$.
\end{proof}

\begin{lem}\label{lem:subsample-sv}
Let $A$ be an expectation matrix of $\hsbm(B,P,f)$ with $k$ blocks with minimum block size $s \geq 16\sqrt{n \log n}$, and let $C$ be the submatrix of $A$ with rows $Y$ and columns $Z$, where $|Y| = \frac{n}{2}$ and $|Z| = \frac{n}{4}$ are chosen randomly from $[n]$ such that $Y \cap Z = \emptyset$. Then, with probability at least $1-\frac{1}{n}$, for all $1 \leq i \leq k$, we have
\[
(\tfrac 1 8 - \tfrac{\sqrt{n \log n}} s)\sigma_i(A_1) \leq \sigma_i(A_1) \leq (\tfrac 1 8 + \tfrac{\sqrt{n \log n}} s)\sigma_i(A_1)
\]
\end{lem}
\begin{proof}
Observe that the blocks in $C$ are indexed in rows by $B_1 \cap Y, \ldots, B_k \cap Y$ and in columns by $B_1 \cap Z, \ldots, B_k \cap Z$. By Chernoff's bound, with probability at least $1 - \frac{1}{n^2}$, we have for all $i$ that
\[
    \frac{1}{2} - \frac{\sqrt{n \log n}}{|B_i|} \leq \frac{|B_i \cap Y|}{|B_i|} \leq \frac{1}{2} + \frac{\sqrt{n \log n}}{|B_i|} \ \ \ \ \ \ \ \ \ \frac 1 4 - \frac{\sqrt{n \log n}}{|B_i|} \leq \frac{|B_i \cap Z|}{|B_i|} \leq \frac 1 4 + \frac{\sqrt{n \log n}}{|B_i|}.
\]

We have $\sigma_{k}(A) = \min_{\text{rank}(A_{k-1}) = k-1} \|A - A_{k-1}\|_F$ and $\sigma_{k}(C) = \min_{\text{rank}(C_{k-1}) = k-1} \|C-C_{k-1}\|_F$; let $A_{k-1}^*$ and $C_{k-1}^*$ be the maximizers of the previous expressions. Let $A'$ denote the matrix $C_{k-1}^*$ with rows and columns duplicated such that each element $(A')_{ij}$ is equal to $(C_{k-1}^*)_{xy}$, where $x,y$ are any two points in the same block as $i,j$, respectively. Accounting for the duplication factors of each block, we have
\[
    \left(\frac 1 2 - \frac{\sqrt{n \log n}}{s}\right)\left(\frac 1 4 - \frac{\sqrt{n \log n}}{s}\right)\|A - A'\|_F \leq  \|C - C_{k-1}^*\|_F,
\]
and thus we see that $(\frac 1 8 - \frac{\sqrt{n \log n}}{s})\sigma_k(A) \leq \sigma_k(A_1)$. By a similar sampling argument, we can show that $(\frac 1 8 + \frac{\sqrt{n \log n}}{s})\sigma_k(A) \geq \sigma_k(A_1)$. Repeating the argument for $\sqrt{\sigma_{i}(A)^2 + \cdots \sigma_{k}(A)^2} = \min_{\text{rank}(A_{i-1})=i-1}\|A - A_{i-1}\|_F$, we obtain the result for all $1 \leq i \leq k$.
\end{proof}
Let $A_1, A_2$ be the expectation matrices of $\hat{A}_1, \hat{A}_2$ for fixed $Y,Z_2$. Using Lemmas~\ref{lem:rand-mat-sv} and~\ref{lem:subsample-sv}, we have that $\sigma_{1}(\hat{A}_2) \leq \sigma_1(A_2) + 4\tau \sqrt{n} + 4 \log n \leq (\frac{1}{8} + \frac{\sqrt{n \log n}}{s})\sigma_1(A) + 4\tau \sqrt{n} + 4 \log n \leq \frac{3}{32} \sigma_1(A) + 4\tau \sqrt{n} + 4 \log n$.
Applying these again, we obtain
\begin{align*}
    d_k(\hat{A}_1) &= \sigma_{k}(\hat{A}_1) - \sigma_{k+1}(\hat{A}_1) \\
    &\geq \sigma_k(A_1) - \sigma_{k+1}(A_1) - 8\tau \sqrt{n} - 8\log n \\
    &\geq (\frac{1}{8}-\frac{\sqrt{n \log n}}{s})(\sigma_k(A) - \sigma_{k+1}(A)) - 8\tau \sqrt{n} - 8\log n \\
    &\geq \frac{1}{16}\sigma_k(A) - 8\tau \sqrt{n} - 8\log n
\end{align*}
Finally, we have $\tilde{\Gamma} = \frac{\tilde{\sigma}_1(\hat{A_2})}{\tilde{d}_k(\hat{A_1})}$, which with probability at least $\delta$, will satisfy
\[
\tilde{\Gamma} \leq \frac{\sigma_1(\hat{A_2}) + \frac{8}{\epsilon} \ln \frac{4}{\delta} }{d_k(\hat{A}_1) - \frac{16}{\epsilon} \ln \frac{4}{\delta}} \leq 
\frac{\frac{3}{32} \sigma_1(A) + 4\tau \sqrt{n} + 4 \log n + \frac{8}{\epsilon} \ln \frac{4}{\delta} }{\frac{1}{16}d_k(A) - 8\tau \sqrt{n} - 8\log n - \frac{16}{\epsilon} \ln \frac{4}{\delta}}.
\]
By our assumption that $\sigma_k(A) \geq 1024 \max \{\tau \sqrt{n}, \frac{1}{\epsilon} \ln \frac{4}{\delta} \}$, we obtain that $\hat{\Gamma} \leq 4 \frac{\sigma_1(A)}{\sigma_k(A)}$, 
This implies that 
\[
    K \leq \frac{12k \sqrt{m\ln \frac{5}{\delta} \ln n}}{\epsilon} \frac{\sigma_1(A)}{\sigma_k(A)} = \frac{48k \sqrt{2 \ln \frac{2n}{\delta} \ln \frac{5}{\delta} \ln n}}{\epsilon} \frac{\sigma_1(A)}{\sigma_k(A)}
    \leq \frac{96k (\ln \frac{5}{\delta})^{3/2}}{\epsilon} \frac{\sigma_1(A)}{\sigma_k(A)},
\]
where the last step follows because $\delta < \frac{1}{n}$. From our assumption, we have $2K \leq 0.1\Delta$, and the result follows.

\subsection{Proof of Corollary~\ref{cor:com-hsbm-util}}

In this special case, we can write $A = P \otimes \textbf{1}_B$, where $P$ is a $k \times k$ matrix with $p$ on the diagonal and $q$ everywhere else, $\textbf{1}_s$ is a $s \times s$ matrix consisting of all $1$s, and $\otimes$ denotes the Kronecker product. It is easy to see that the eigenvalues of $P$ are $\{p + q(k-1), p-q, \ldots, p-q\}$, and the eigenvalues of $\textbf{1}_s$ are $\{s, 0, \ldots, 0\}$. The eigenvalues of $A$ are the product of the two sets of eigenvalues of $P$ and $\textbf{1}_s$. Thus, the top $k$ largest eigenvalues are $s(p + q(k-1))$ and then $k-1$ copies of $s(p-q)$.

Thus, the following properties of $A$ hold: (1) $\sigma_1 = s(p + q(k-1)) ] \leq sk (p + q)$, (2) $\sigma_k = s(p-q)$, (3) $\tau = \sqrt{p}$, and (4) $\Delta = (p-q)\sqrt{s}$. We are able to apply Theorem~\ref{thm:com-hsbm-util} when 
\begin{align*}
    (p-q) \sqrt{s} &\geq \frac{s(p+q)}{s(p-q)} \frac{Ck(\log \frac{1}{\delta})^{3/2}}{\epsilon} \\
    \frac{(p-q)^2}{p+q} &\geq \frac{C(k\log \frac{1}{\delta})^{3/2}}{\sqrt{n}}.
\end{align*}
This establishes the result.