Hierarchical Clustering is a staple of unsupervised machine learning with more than 60 years of history~\citep{ward1963hierarchical}.
%and numerous applications~\cite{leskovec2014mining,diez2015novel,tumminello2010correlation,dasgupta2016cost}. 
Contrary to {\it flat} clustering methods (such as $k$-means,~\citet{J10}), which provide a single partitioning of the data, {\it hierarchical} clustering algorithms produce a recursive refining of the partitions into increasingly fine-grained clusters. 
%More formally, a hierarchical clustering algorithm is given as input an arbitrary set $V$ of items, and a (arbitrary) notion of similarity (or dissimilarity) between them encoded as a weighted undirected graph $G=(V, E, w)$. Given this input, a hierarchical clustering algorithm outputs an unsupervised representation of the dataset as a tree (a.k.a. dendrogram) where the leaves  correspond to the input data points $V$ and the internal nodes of the tree correspond to the clusters at the different levels of granularities. 
%The objective of the tree, intuitively, is to cluster together the most similar items in the lowest possible clusters, while separating dissimilar items as high as possible in the tree.
The clustering process can be described by a tree (or dendrogram), and the objective of the tree is to cluster the most similar items in the lowest possible clusters, while separating dissimilar items as high as possible.

The versatility of such methods is apparent from the widespread use of hierarchical clustering in disparate areas of science, such as social networks analysis~\citep{leskovec2014mining,mann2008use}, bioinformatics~\citep{diez2015novel},  phylogenetics~\citep{sneath1962numerical,jardine1968model}, gene expression analysis~\citep{eisen1998cluster}, text classification~\citep{steinbach2000} and  finance~\citep{tumminello2010correlation}. Popular hierarchical clustering methods (such as linkage~\citep{J10}) are commonly available in standard scientific computing packages~\citep{virtanen2020scipy} as well as large-scale  production systems~\citep{bateni2017affinity, dhulipala2022hierarchical}.

Despite the fact that many of these applications involve private and sensitive user data, all research on hierarchical clustering (with few exceptions~\citep{kolluri2021private, xiao2014differentially} discussed later) has ignored the problem of defining {\it  privacy-preserving} algorithms. In particular, to the best of our knowledge, no work has provided {\it differentially-private (DP)}~\citep{dwork2014algorithmic} algorithms for hierarchical clustering with provable approximation guarantees. 

In this work, we seek to address this limitation by advancing the study of differentially-private approximation algorithms for hierarchical clustering under the rigorous optimization framework introduced by~\citet{dasgupta2016cost}. This celebrated framework introduces an objective function for hierarchical clustering (see Section~\ref{chap5-sec:preliminaries} for a formal definition) formalizing the goal of clustering similar items lower in the tree. 

%We provide approximation algorithms for the Dasgupta's cost function, while respecting a popular definition of differential privacy (DP) for graph data, known as edge-level DP~\cite{marc2021differentially,eliavs2020differentially, epasto2022differentially}. In this definition, two undirected graphs $G=(V,E,w')$ and $G'=(V,E',w')$, are {\it neighbors} if they differ only in the presence (or weight) of a single edge. An algorithm $\mathcal{A}$ is edge-level $\epsilon$-differentially private if the difference in the probability of observing any particular outcome from the algorithm when run on $G$ vs $G'$ is bounded: $\Pr[\mathcal{A}(G) \in S] \leq e^{\epsilon}\cdot \Pr[\mathcal{A}(G') \in S].$

%Algorithms respecting edge-level differential privacy, promise a strong notion of plausible deniability for the input data: an adversary observing the hierarchical clustering is information-theoretically bounded in the ability to determine the input similarity between any two items. This is especially relevant when the input graph represents private (or sensitive) user information (for example in social networks,~\citet{leskovec2014mining}). 

Our algorithms are edge-level {\em Differentially Private (DP)} on an input similarity graph, which is relevant when edges of the input graph represents sensitive user information.
Designing an edge-level DP algorithm requires proving that the algorithm is insensitive to changes to a single edge of the similarity graph. As we shall see, this is especially challenging for hierarchical clustering. In fact, commonly-used hierarchical clustering algorithms (such as linkage-based ones~\citep{J10}) are {\it deterministically} sensitive to a single edge, thus leaking directly the input edges. Moreover, as we show, strong inapproximability bounds exist for Dasgupta's objective under differential privacy, highlighting the technical difficulty of the problem.

\paragraph{Main contributions}
First, we show in Section~\ref{chap5-sec:lower-bounds} that no edge-level $\epsilon$-DP algorithm (even with exponential time) exists for Dasgupta's objective with less than $O(|V|^2/ \epsilon)$ additive error. This prevents defining private algorithms with meaningful approximation guarantees for {\it arbitrary} sparse graphs.

Second, on the positive side, we provide the first polynomial time, edge-level approximation algorithm for Dasguta's objective with $ O(|V|^{2.5} /\epsilon)$ additive error and multiplicative error matching that of the best non-private algorithm~\citep{agarwal2022sublinear}. This algorithm is based on recent advances in private cut sparsifiers~\citep{eliavs2020differentially}. Moreover, we show an (exponential time) algorithm with $O(|V|^{2} \log n/ \epsilon)$ additive error, almost matching the lower bound.

Third, given the strong lower bounds, in Section~\ref{chap5-sec:algorithms-hsbm} we focus on a popular model of graphs with a planted hierarchical clustering based on the {\em Stochastic Block Model (SBM)}~\citep{cohen2017hierarchical}. For such graphs, we present a private $1+o(1)$ approximation algorithm recovering almost exactly the hierarchy on the blocks. Our algorithm uses, as a black-box, any reconstruction algorithm for the stochastic block model. 

Fourth, we introduce a practical and efficient DP SBM community reconstruction algorithm (Section~\ref{chap5-sec:algorithms-hsbm}). This algorithm is based on perturbation theory of graph spectra combined with dimensionality reduction to avoid adding high noise in the Gaussian mechanism. Combined with our clustering algorithm, this results in the first private approximation algorithm for hierarchical clustering in the hierarchical SBM. 

Finally, we show in Section~\ref{chap5-sec:experiments} that this algorithm can be efficiently implemented and works well in practice.

\section{Related Work}
\label{chap5-sec:related}

Our work spans the areas of differential privacy, hierarchical clustering and community detection in stochastic block model. For a complete discussion, see Appendix~\ref{chap5-app:related}.

\paragraph{Graph algorithms under DP}
Differential privacy~\citep{dwork2006calibrating} has recently the gold standard of privacy. We refer to~\citet{dwork2014algorithmic} for a survey. Relevant to this work is the area of differential privacy in graphs. Definitions based on edge-level~\citep{epasto2022differentially,eliavs2020differentially} and node-level~\citep{kasiviswanathan2013analyzing} privacy have been proposed. The most related work is that on graph cut approximation~\citep{eliavs2020differentially,arora2019differentially}, as well as that of private correlation clustering~\citep{bun2021differentially, cohen2022near}.

\paragraph{Hierarchical Clustering}
Until recently, most work on hierarchical clustering were heuristic in nature, with the most well-known being the linkage-based ones~\citep{J10,bateni2017affinity}. \citet{dasgupta2016cost} introduced a combinatorial objective for hierarchical clustering which we study in this paper. Since this work, many authors have designed algorithms for variants of the problem with no privacy~\citep{cohen2017hierarchical,cohen2019hierarchical, charikar2017approximate,moseley2017approximation, agarwal2022sublinear,chatziafratis2020bisect}.

Limited work has been devoted to DP hierarchical clustering algorithms. One paper~\citep{xiao2014differentially} initiates private clustering via MCMC methods, which are not guaranteed to be polynomial time. Follow-up work~\citep{kolluri2021private} shows that sampling from the Boltzmann distribution (essentially the exponential mechanism~\citep{mcsherry2007mechanism} in DP) produces an approximation to the maximization version of Dasgupta's function, which is a different problem formulation. Again, this algorithm is not provably polynomial time.

\paragraph{Private flat clustering}
Contrary to hierarchical clustering, the area of private {\it flat} clustering on metric spaces has received large attention. Most work in this area has focused on improving the privacy-approximation trade-off~\citep{ghazi2020differentially,balcan2017differentially} and on efficiency~\citep{hegde2021sok,cohennear,cohen2022scalable}.

\paragraph{Stochastic block models}

The Stochastic Block Model (SBM) is a classic model for random graphs with planted partitions which has received a significant attention in the literature~\citep{MR3520025-Guedon16,montanari2016semidefinite, moitra2016robust,MR4115142,ding2022robust,Liu-Moitra-minimax}. For our work, we focus on a variant which has nested ground-truth communities arranged in hierarchical fashion. This model has received attention for hierarchical clustering~\citep{cohen2017hierarchical}.   

The study of private algorithms for SBMs is instead very recent. One of the only results known for private (non-hierarchical) SBMs is the work of~\citet{seif2022differentially} which provides quasi-polynomial time community detection algorithms for some regimes of the model.
Finally, concurrently to our work, the manuscript of~\citet{chen2023private} provides strong approximation guarantees using semi-definite programming for recovering SBM communities. Community detection is a distinct problem from hierarchical clustering, and this work is independent of ours. 

The connection between existing work in the SBM and ours is that,
in Section~\ref{chap5-sec:algorithms-hsbm}, we design a hierarchical clustering algorithm (Algorithm~\ref{chap5-alg:priv-hc-hsbm}) which uses community detection as a black-box. Moreover, we show a novel algorithm for hierarchical SBM community detection (Algorithm~\ref{chap5-alg:priv-hsbm}), independent of~\citet{chen2023private}, which is of practical interest because it uses SVDs, instead of semidefinite programming, and thus does not have a large polynomial run-time.
